{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuKBuwU15caxkZMTRulvnm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/I51GMA/mlproject/blob/main/HMM_POS_Tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from numba import njit\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCele34D0YMg",
        "outputId": "562b786d-b9ea-4556-ad63-e0b01ae10950"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load word and tag data from Brown corpus\n",
        "word_and_tag = list(brown.tagged_words())\n",
        "\n",
        "# Extract unique words and POS tags\n",
        "words = list(set([word for word, _ in word_and_tag]))\n",
        "pos_tags = list(set([tag for _, tag in word_and_tag]))\n",
        "\n",
        "# Mapping words and POS tags to indices\n",
        "word_to_index = {word: idx for idx, word in enumerate(words)}\n",
        "tag_to_index = {tag: idx for idx, tag in enumerate(pos_tags)}\n",
        "\n",
        "# Convert word-tag pairs into their corresponding indices before passing to JIT function\n",
        "indexed_word_tag_pairs = [\n",
        "    (word_to_index[word], tag_to_index[tag])\n",
        "    for word, tag in word_and_tag\n",
        "]\n",
        "\n",
        "@njit  # Use Numba to optimize the computation\n",
        "def make_sparse_matrix(indexed_word_tag_pairs, num_words, num_tags):\n",
        "    sparse_matrix = np.zeros((num_words, num_tags))\n",
        "    for word_idx, tag_idx in indexed_word_tag_pairs:\n",
        "        sparse_matrix[word_idx, tag_idx] += 1\n",
        "    return sparse_matrix"
      ],
      "metadata": {
        "id": "x61W5SDi0uRd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnm = True\n",
        "\n",
        "sparse_matrix_path = './sparse_matrix.csv'\n",
        "\n",
        "# If the CSV exists or we need to reconstruct the matrix\n",
        "if not os.path.exists(sparse_matrix_path) or bnm:\n",
        "    sparse_matrix = make_sparse_matrix(indexed_word_tag_pairs, len(words), len(pos_tags))\n",
        "\n",
        "    df = pd.DataFrame(sparse_matrix, columns=pos_tags, dtype=np.int32)\n",
        "    df.insert(0, 'Words', words)\n",
        "    df.to_csv(sparse_matrix_path, index=False)\n",
        "else:\n",
        "    df = pd.read_csv(sparse_matrix_path)"
      ],
      "metadata": {
        "id": "gtu46AXL2g-G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_transition_matrix_and_initial_probs(tagged_sentences, tag_to_index):\n",
        "    num_tags = len(tag_to_index)\n",
        "\n",
        "    # Initializing the transition matrix and initial probabilities array\n",
        "    transition_matrix = np.zeros((num_tags, num_tags), dtype=np.float64)\n",
        "    initial_probs = np.zeros(num_tags, dtype=np.float64)\n",
        "\n",
        "    for sentence in tagged_sentences:\n",
        "        # Get the index of the first tag in the sentence for initial probabilities\n",
        "        initial_probs[tag_to_index[sentence[0][1]]] += 1\n",
        "\n",
        "        for n in range(len(sentence) - 1):\n",
        "            tag_i = sentence[n][1]\n",
        "            tag_j = sentence[n + 1][1]\n",
        "\n",
        "            transition_matrix[tag_to_index[tag_i], tag_to_index[tag_j]] += 1\n",
        "\n",
        "    # Normalize the transition matrix row-wise, handling cases where the row sums to 0\n",
        "    row_sums = np.sum(transition_matrix, axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1  # Avoid division by zero for tags with no transitions\n",
        "    transition_matrix /= row_sums\n",
        "\n",
        "    # Normalize initial probabilities, handling zero-sum case\n",
        "    if np.sum(initial_probs) > 0:\n",
        "        initial_probs /= np.sum(initial_probs)\n",
        "\n",
        "    return transition_matrix, initial_probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw0xVyP77nsp",
        "outputId": "0d0faa08-4eb8-47f9-aadb-4f9206388024"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transition Matrix (A):\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Initial Probabilities (Pi):\n",
            " [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.00021631 0.         0.         0.         0.\n",
            " 0.         0.00043262 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.00021631 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.00043262 0.\n",
            " 0.         0.         0.         0.00021631 0.00021631 0.\n",
            " 0.         0.00021631 0.         0.00021631 0.0045425  0.\n",
            " 0.00259572 0.00043262 0.         0.         0.00064893 0.\n",
            " 0.         0.03634004 0.         0.         0.         0.\n",
            " 0.         0.         0.00043262 0.         0.00021631 0.0311486\n",
            " 0.         0.         0.00519143 0.         0.00021631 0.\n",
            " 0.         0.         0.         0.00108155 0.         0.00086524\n",
            " 0.01492537 0.         0.         0.         0.         0.00540774\n",
            " 0.         0.         0.00021631 0.         0.         0.00021631\n",
            " 0.         0.00346096 0.00151417 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.00367727 0.00043262 0.00129786 0.         0.         0.\n",
            " 0.         0.00129786 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.02336145 0.         0.         0.         0.03915207 0.\n",
            " 0.         0.00129786 0.         0.         0.         0.\n",
            " 0.00129786 0.         0.         0.         0.         0.00043262\n",
            " 0.         0.         0.         0.         0.00043262 0.\n",
            " 0.00259572 0.         0.         0.         0.         0.\n",
            " 0.         0.01189704 0.         0.         0.00021631 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.00043262 0.02401038 0.00129786 0.         0.\n",
            " 0.         0.         0.0177374  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.00021631 0.\n",
            " 0.00367727 0.         0.         0.         0.00064893 0.\n",
            " 0.         0.01449275 0.         0.         0.         0.\n",
            " 0.01427644 0.         0.         0.00043262 0.         0.\n",
            " 0.00043262 0.00237941 0.         0.         0.         0.\n",
            " 0.         0.         0.00021631 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.00086524 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.00021631 0.         0.         0.00173048 0.00021631\n",
            " 0.         0.         0.         0.         0.00648929 0.\n",
            " 0.00064893 0.         0.05970149 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.00497512 0.\n",
            " 0.         0.         0.         0.00043262 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.00021631 0.00021631 0.         0.00173048 0.00346096\n",
            " 0.         0.         0.         0.         0.02076574 0.\n",
            " 0.         0.         0.00151417 0.         0.         0.00064893\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.00064893 0.         0.01038287 0.         0.         0.\n",
            " 0.         0.00281203 0.         0.01817002 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.00259572 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.00129786 0.01384382 0.\n",
            " 0.         0.         0.00584036 0.         0.         0.\n",
            " 0.         0.         0.00021631 0.02574086 0.00021631 0.\n",
            " 0.         0.         0.         0.         0.         0.00021631\n",
            " 0.         0.         0.         0.         0.         0.07051698\n",
            " 0.00281203 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.00108155 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00021631\n",
            " 0.00173048 0.00627298 0.         0.00021631 0.         0.\n",
            " 0.         0.         0.         0.         0.02119836 0.00021631\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.18883842 0.         0.         0.         0.         0.00108155\n",
            " 0.         0.00064893 0.00064893 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.00021631 0.\n",
            " 0.         0.         0.         0.         0.07397794 0.\n",
            " 0.00021631 0.         0.         0.01081549 0.         0.\n",
            " 0.         0.01622323 0.         0.         0.         0.\n",
            " 0.         0.00410989 0.00043262 0.         0.00086524 0.00021631\n",
            " 0.         0.         0.         0.         0.         0.00021631\n",
            " 0.12610859 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.00259572 0.00129786 0.         0.\n",
            " 0.         0.         0.         0.         0.00043262 0.\n",
            " 0.         0.00173048 0.         0.        ]\n",
            "Emission Matrix (B):\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating transition matrix and initial probabilities array\n",
        "A, Pi = make_transition_matrix_and_initial_probs(brown.tagged_sents(categories='news'), tag_to_index)"
      ],
      "metadata": {
        "id": "Cj1Lv6h58fNh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the emission probabilities\n",
        "B = df.iloc[:, 1:].values.astype(np.float64).T\n",
        "B /= np.sum(B, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "0ooG3z8s842B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import jit\n",
        "\n",
        "@jit(nopython=True)\n",
        "def viterbi_algorithm_log(state_transition_matrix, initial_probs, emission_matrix, observations):\n",
        "    \"\"\"A unique Viterbi algorithm for finding the most likely sequence of hidden states using log probabilities.\n",
        "\n",
        "    Args:\n",
        "        state_transition_matrix (np.ndarray): State transition probability matrix (I x I)\n",
        "        initial_probs (np.ndarray): Initial state distribution (I)\n",
        "        emission_matrix (np.ndarray): Emission matrix (I x K)\n",
        "        observations (np.ndarray): Observation sequence (length N)\n",
        "\n",
        "    Returns:\n",
        "        optimal_states (np.ndarray): Optimal state sequence (length N)\n",
        "        log_probability_matrix (np.ndarray): Log probability matrix\n",
        "        backtrack_matrix (np.ndarray): Backtrack matrix for finding the optimal path\n",
        "    \"\"\"\n",
        "    num_states = state_transition_matrix.shape[0]  # I: Number of states\n",
        "    sequence_length = len(observations)            # N: Length of observation sequence\n",
        "\n",
        "    # Tiny value to prevent log(0) errors\n",
        "    tiny_value = np.finfo(np.float64).tiny\n",
        "\n",
        "    # Take logarithms of all probabilities (handling zero probabilities)\n",
        "    log_transition_matrix = np.log(state_transition_matrix + tiny_value)\n",
        "    log_initial_probs = np.log(initial_probs + tiny_value)\n",
        "    log_emission_matrix = np.log(emission_matrix + tiny_value)\n",
        "\n",
        "    # Initialize log probability and backtrack matrices\n",
        "    log_probability_matrix = np.zeros((num_states, sequence_length), dtype=np.float64)\n",
        "    backtrack_matrix = np.zeros((num_states, sequence_length - 1), dtype=np.int32)\n",
        "\n",
        "    # Initialize first column of the log probability matrix\n",
        "    log_probability_matrix[:, 0] = log_initial_probs + log_emission_matrix[:, observations[0]]\n",
        "\n",
        "    # Dynamic programming: fill in the log probability and backtrack matrices\n",
        "    for t in range(1, sequence_length):\n",
        "        for current_state in range(num_states):\n",
        "            log_transition_values = log_transition_matrix[:, current_state] + log_probability_matrix[:, t - 1]\n",
        "            log_probability_matrix[current_state, t] = np.max(log_transition_values) + log_emission_matrix[current_state, observations[t]]\n",
        "            backtrack_matrix[current_state, t - 1] = np.argmax(log_transition_values)\n",
        "\n",
        "    # Backtracking to find the optimal state sequence\n",
        "    optimal_states = np.zeros(sequence_length, dtype=np.int32)\n",
        "    optimal_states[-1] = np.argmax(log_probability_matrix[:, -1])\n",
        "\n",
        "    for t in range(sequence_length - 2, -1, -1):\n",
        "        optimal_states[t] = backtrack_matrix[optimal_states[t + 1], t]\n",
        "\n",
        "    return optimal_states, log_probability_matrix, backtrack_matrix"
      ],
      "metadata": {
        "id": "F0nhQPR-4WK0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = 203\n",
        "test_sent = brown.sents(categories='news')[r]\n",
        "\n",
        "O = []\n",
        "correct_tag_seq = []\n",
        "test_tagged = brown.tagged_sents(categories='news')[r]\n",
        "\n",
        "for word, tag in test_tagged:\n",
        "    if word in word_to_index:\n",
        "        O.append(word_to_index[word])\n",
        "        if tag in tag_to_index:\n",
        "            correct_tag_seq.append(tag_to_index[tag])\n",
        "\n",
        "O = np.array(O, dtype=np.int32)\n",
        "\n",
        "# Performing Viterbi decoding using the Viterbi function\n",
        "opt_state_seq, log_prob_trellis, backtrack_matrix = viterbi_algorithm_log(A, Pi, B, O)\n",
        "\n",
        "print('Observation sequence:   O  = ', O)\n",
        "print('Optimal state sequence: S  = ', opt_state_seq)\n",
        "print('Correct state sequence: S* = ', correct_tag_seq)\n",
        "print(\"Do they match: \", correct_tag_seq == list(opt_state_seq))\n",
        "\n",
        "guessed_tags = [pos_tags[i] for i in opt_state_seq]\n",
        "test_tagged_words = [word for word, _ in test_tagged if word in word_to_index]\n",
        "test_result_df = pd.DataFrame(index=test_tagged_words, columns=['Correct', 'Guessed'], data=zip(test_tagged, guessed_tags)).T\n",
        "\n",
        "print('The sentence: ', test_sent)\n",
        "print(test_result_df.iloc[:, (test_result_df.nunique() != 1).values])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuytWWC69SCF",
        "outputId": "1392c3af-cf43-4a20-bcda-38305479e7c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation sequence:   O  =  [17724 15056 34685 22511 10822   191  3607  3463 15922 42426  4217 16795\n",
            " 14637 11131   636 52778 32302 53709 24256 21729  5357]\n",
            "Optimal state sequence: S  =  [359 190 309 325 338 402 309  71 429 439 424 402 345  89 345 157 456  61\n",
            " 345 300  13]\n",
            "Correct state sequence: S* =  [359, 190, 309, 325, 338, 402, 309, 71, 429, 439, 424, 402, 345, 115, 345, 157, 456, 61, 345, 300, 13]\n",
            "Do they match:  False\n",
            "The sentence:  ['It', 'is', 'impossible', 'to', 'get', 'a', 'fair', 'trial', 'when', 'some', 'of', 'the', 'defendants', 'made', 'statements', 'involving', 'themselves', 'and', 'others', \"''\", '.']\n",
            "                It         is        impossible        to        get        a  \\\n",
            "Correct  (It, PPS)  (is, BEZ)  (impossible, JJ)  (to, TO)  (get, VB)  (a, AT)   \n",
            "Guessed        PPS        BEZ                JJ        TO         VB       AT   \n",
            "\n",
            "               fair        trial         when         some  ...        the  \\\n",
            "Correct  (fair, JJ)  (trial, NN)  (when, WRB)  (some, DTI)  ...  (the, AT)   \n",
            "Guessed          JJ           NN          WRB          DTI  ...         AT   \n",
            "\n",
            "                defendants         made         statements         involving  \\\n",
            "Correct  (defendants, NNS)  (made, VBD)  (statements, NNS)  (involving, VBG)   \n",
            "Guessed                NNS          VBN                NNS               VBG   \n",
            "\n",
            "                 themselves        and         others        ''       .  \n",
            "Correct  (themselves, PPLS)  (and, CC)  (others, NNS)  ('', '')  (., .)  \n",
            "Guessed                PPLS         CC            NNS        ''       .  \n",
            "\n",
            "[2 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import KFold\n",
        "# from nltk.corpus import brown\n",
        "# import numpy as np\n",
        "\n",
        "# # Define number of folds\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# # Convert all sentences and tags to a suitable format for cross-validation\n",
        "# all_sentences = brown.sents(categories='news')\n",
        "# all_tagged_sentences = brown.tagged_sents(categories='news')\n",
        "\n",
        "# # Convert the words and tags into their corresponding indices for cross-validation\n",
        "# all_word_sequences = []\n",
        "# all_tag_sequences = []\n",
        "\n",
        "# for sent, tagged_sent in zip(all_sentences, all_tagged_sentences):\n",
        "#     word_sequence = [word_to_index[word] for word in sent if word in word_to_index]\n",
        "#     tag_sequence = [tag_to_index[tag] for _, tag in tagged_sent if tag in tag_to_index]\n",
        "\n",
        "#     if word_sequence and tag_sequence and len(word_sequence) == len(tag_sequence):\n",
        "#         all_word_sequences.append(word_sequence)\n",
        "#         all_tag_sequences.append(tag_sequence)\n",
        "\n",
        "# # Function to recalculate transition and emission matrices based on the training data\n",
        "# def calculate_transition_and_emission_matrices(train_word_sequences, train_tag_sequences):\n",
        "#     # Count tag transitions and tag emissions\n",
        "#     transition_matrix = np.zeros((len(tag_to_index), len(tag_to_index)), dtype=np.float64)\n",
        "#     initial_probs = np.zeros(len(tag_to_index), dtype=np.float64)\n",
        "#     emission_matrix = np.zeros((len(tag_to_index), len(word_to_index)), dtype=np.float64)\n",
        "\n",
        "#     for word_seq, tag_seq in zip(train_word_sequences, train_tag_sequences):\n",
        "#         # Update initial probabilities\n",
        "#         initial_probs[tag_seq[0]] += 1\n",
        "\n",
        "#         # Update transition and emission matrices\n",
        "#         for i in range(len(tag_seq)):\n",
        "#             emission_matrix[tag_seq[i], word_seq[i]] += 1\n",
        "#             if i > 0:\n",
        "#                 transition_matrix[tag_seq[i-1], tag_seq[i]] += 1\n",
        "\n",
        "#     # Normalize transition and emission matrices\n",
        "#     row_sums_transition = np.sum(transition_matrix, axis=1, keepdims=True)\n",
        "#     row_sums_transition[row_sums_transition == 0] = 1\n",
        "#     transition_matrix /= row_sums_transition\n",
        "\n",
        "#     row_sums_emission = np.sum(emission_matrix, axis=1, keepdims=True)\n",
        "#     row_sums_emission[row_sums_emission == 0] = 1\n",
        "#     emission_matrix /= row_sums_emission\n",
        "\n",
        "#     # Normalize initial probabilities\n",
        "#     initial_probs /= np.sum(initial_probs)\n",
        "\n",
        "#     return transition_matrix, initial_probs, emission_matrix\n",
        "\n",
        "# # Perform 5-fold cross-validation\n",
        "# accuracies = []\n",
        "\n",
        "# for train_index, test_index in kf.split(all_word_sequences):\n",
        "#     # Split data into training and testing sets\n",
        "#     train_word_sequences = [all_word_sequences[i] for i in train_index]\n",
        "#     train_tag_sequences = [all_tag_sequences[i] for i in train_index]\n",
        "#     test_word_sequences = [all_word_sequences[i] for i in test_index]\n",
        "#     test_tag_sequences = [all_tag_sequences[i] for i in test_index]\n",
        "\n",
        "#     # Recalculate transition and emission matrices based on training data\n",
        "#     A, Pi, B = calculate_transition_and_emission_matrices(train_word_sequences, train_tag_sequences)\n",
        "\n",
        "#     # For each test sentence, perform Viterbi decoding and calculate accuracy\n",
        "#     total_tags = 0\n",
        "#     correct_tags = 0\n",
        "\n",
        "#     for test_words, correct_tags_seq in zip(test_word_sequences, test_tag_sequences):\n",
        "#         test_words = np.array(test_words, dtype=np.int32)\n",
        "\n",
        "#         # Perform Viterbi decoding on the test sentence\n",
        "#         predicted_tags, _, _ = viterbi_algorithm_log(A, Pi, B, test_words)\n",
        "\n",
        "#         # Compare predicted tags with correct tags and compute accuracy\n",
        "#         correct_tags += np.sum(np.array(predicted_tags) == np.array(correct_tags_seq))\n",
        "#         total_tags += len(correct_tags_seq)\n",
        "\n",
        "#     # Calculate accuracy for this fold\n",
        "#     fold_accuracy = correct_tags / total_tags\n",
        "#     accuracies.append(fold_accuracy)\n",
        "#     print(f\"Fold accuracy: {fold_accuracy:.4f}\")\n",
        "\n",
        "# # Output final average accuracy across all folds\n",
        "# final_accuracy = np.mean(accuracies)\n",
        "# print(f\"Final 5-fold cross-validation accuracy: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "7FNAGgRDXdzC",
        "outputId": "15a184b0-ab17-4dbf-91a2-443312baee42"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold accuracy: 0.8946\n",
            "Fold accuracy: 0.8964\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "CPUDispatcher(<function viterbi_algorithm_log at 0x7ecf7a4abeb0>) returned a result with an exception set",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/serialize.py\u001b[0m in \u001b[0;36m_numba_unpickle\u001b[0;34m(address, bytedata, hashed)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_numba_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytedata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \"\"\"Used by `numba_unpickle` from _helperlib.c\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;31mSystemError\u001b[0m: _PyEval_EvalFrameDefault returned a result with an exception set",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;31mSystemError\u001b[0m: _PyEval_EvalFrameDefault returned a result with an exception set",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-8905ba3613c1>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Perform Viterbi decoding on the test sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mpredicted_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviterbi_algorithm_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Compare predicted tags with correct tags and compute accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: CPUDispatcher(<function viterbi_algorithm_log at 0x7ecf7a4abeb0>) returned a result with an exception set"
          ]
        }
      ]
    }
  ]
}